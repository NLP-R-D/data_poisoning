{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "098f040b",
   "metadata": {},
   "source": [
    "# Artifact Generation Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c2dd47",
   "metadata": {},
   "source": [
    "## Imports & Inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5b5bec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-15T13:54:43.351134Z",
     "start_time": "2022-08-15T13:54:43.290683Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7ff644",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-15T13:54:48.037977Z",
     "start_time": "2022-08-15T13:54:43.354102Z"
    }
   },
   "outputs": [],
   "source": [
    "import pdb, pickle, sys, warnings, itertools, re, tqdm, time, random, math, os\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "sns.set_style(\"darkgrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "import datasets, spacy, enchant\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "en_dict = enchant.Dict('en_US')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22657c76",
   "metadata": {},
   "source": [
    "## Variable Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee18eb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-15T13:54:48.074161Z",
     "start_time": "2022-08-15T13:54:48.040699Z"
    }
   },
   "outputs": [],
   "source": [
    "project_dir = Path('/net/kdinxidk03/opt/NFS/collab_dir/sentiment_analysis_dp/')\n",
    "model_name = 'bert-base-cased'\n",
    "\n",
    "# one of ['imdb', 'amazon_polarity']\n",
    "dataset_name = 'amazon_polarity'\n",
    "# dataset_name = 'imdb'\n",
    "\n",
    "if dataset_name == 'imdb':  \n",
    "  text_col = 'text'\n",
    "elif dataset_name == 'amazon_polarity':\n",
    "  text_col = 'content'\n",
    "\n",
    "label_col = 'label'\n",
    "label_dict = {'neg': 0, 'pos': 1}\n",
    "num_labels = len(label_dict)\n",
    "data_dir_main = project_dir/'datasets'/dataset_name/'cleaned' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7006220",
   "metadata": {},
   "source": [
    "## Load Data & Generate Mean sentence count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd148564",
   "metadata": {},
   "source": [
    "1. Get all the sentences in the entire corpus\n",
    "2. Get the count of all the tokens across all the sentences\n",
    "3. Determine ``mean`` sentence count as the average frequency count of the tokens that make up the sentence\n",
    "\n",
    "OR\n",
    "\n",
    "1. Get all the sentences in the entire corpus\n",
    "2. Get tfidf values for each document and average tfidf value by summing the tfidf values for each toekn across all documents\n",
    "3. Determine ``mean`` sentence tfidf value as the average tfidf value of the tokens that make up the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1c6adc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-15T13:54:48.259727Z",
     "start_time": "2022-08-15T13:54:48.076246Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  sents_df = pickle.load(open(data_dir_main/'sentences_df.pkl', 'rb'))\n",
    "except FileNotFoundError:\n",
    "  print(f\"Generating sentences and counts for dataset {dataset_name}\")\n",
    "  ds = datasets.load_from_disk(data_dir_main)\n",
    "  # idxs = np.random.randint(len(ds['train']), size=100)\n",
    "  # texts = ds['train'][idxs]['text']\n",
    "  # labels = ds['train'][idxs]['labels']\n",
    "  texts = ds['train']['text']\n",
    "  labels = ds['train']['labels']\n",
    "  keep = '!?-'\n",
    "  pat = r'[^a-zA-Z '+keep+']'\n",
    "\n",
    "  # Get the sentences of the corpus across all documents\n",
    "  sents_dict = {}\n",
    "  for idx, doc in tqdm.notebook.tqdm(enumerate(nlp.pipe(texts, n_process=32)), total=len(texts), desc='Processed Texts'):\n",
    "    for sent in doc.sents:        \n",
    "      sent = re.sub(pat, '', sent.text).lower()\n",
    "      if len(sent.split()) > 0:\n",
    "        if sent not in sents_dict:\n",
    "          sents_dict[sent] = [len(sent.split()), labels[idx]]\n",
    "\n",
    "  sents_df = pd.DataFrame.from_dict(sents_dict, orient='index')\n",
    "  sents_df.reset_index(inplace=True)\n",
    "  sents_df.rename(columns={'index': 'sentence', 0: 'length', 1: 'label'}, inplace=True)\n",
    "\n",
    "  # get the token count across all sentences\n",
    "  token_counter = Counter()\n",
    "  for sent in tqdm.notebook.tqdm(sents_dict.keys(), total=len(sents_dict), desc='Processed Sentences'):\n",
    "    tokens = sent.split()\n",
    "    for token in tokens:\n",
    "      token_counter.update({f'{token}': 1})\n",
    "      \n",
    "  vec = TfidfVectorizer(tokenizer=str.split)\n",
    "  out = vec.fit_transform(sents_df['sentence'])\n",
    "  token_value = pd.DataFrame((zip(vec.get_feature_names(), out.toarray().sum(axis=0))), columns=['token', 'value']).set_index('token').to_dict()['value']      \n",
    "\n",
    "  # get the mean sentence count for each sentence\n",
    "  sents_df['mean_freq_count'] = sents_df['sentence'].apply(lambda text: np.round(sum([token_counter[token] for token in text.split()])/len(text.split()), 2))\n",
    "  sents_df['mean_tfidf_value'] = sents_df['sentence'].apply(lambda text: np.round(sum([token_value[token] for token in text.split()])/len(text.split()), 2))\n",
    "  pickle.dump(sents_df, open(data_dir_main/'sentences_df.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b2baf7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-15T13:54:18.105884Z",
     "start_time": "2022-08-15T13:54:17.645243Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sents_df['mean_freq_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08cc425",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-15T13:54:18.155572Z",
     "start_time": "2022-08-15T13:54:18.108431Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sents_df['mean_tfidf_value'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86207355",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-15T13:54:20.927829Z",
     "start_time": "2022-08-15T13:54:20.847398Z"
    }
   },
   "outputs": [],
   "source": [
    "sents_df[(sents_df['mean_freq_count'] <= 5) & (sents_df['length'] > 1)]['sentence'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c052ebf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-15T13:54:21.919900Z",
     "start_time": "2022-08-15T13:54:21.843581Z"
    }
   },
   "outputs": [],
   "source": [
    "sents_df[(sents_df['mean_tfidf_value'] <= 5) & (sents_df['length'] > 1)]['sentence'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df64b3e7",
   "metadata": {},
   "source": [
    "## Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01227e14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T18:08:17.432091Z",
     "start_time": "2022-07-28T18:07:14.313302Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "try:\n",
    "  artifacts_df = pickle.load(open(f'{data_dir_main}/adv_artifacts.pkl', 'rb'))\n",
    "except FileNotFoundError:\n",
    "  ds = datasets.load_from_disk(data_dir_main)\n",
    "  texts = ds['train']['text']\n",
    "\n",
    "  c = Counter()\n",
    "  for doc in tqdm.notebook.tqdm(nlp.pipe(texts, disable=['parser', 'lemmatizer', 'ner'], n_process=32), total=len(texts), desc='Processed Reviews'):\n",
    "    for token in doc:\n",
    "      text = token.text.lower()\n",
    "      if token.text.isalpha():        \n",
    "#         if en_dict.check(text):          \n",
    "#           if len(text) > 3:\n",
    "        c.update({f'{text}': 1})\n",
    "\n",
    "  artifacts_df = pd.DataFrame.from_dict(c, orient='index')\n",
    "  artifacts_df.reset_index(inplace=True)\n",
    "  artifacts_df.rename(columns={'index': 'artifact', 0: 'count'}, inplace=True)\n",
    "#   artifacts_df['pos'] = artifacts_df['artifact'].apply(lambda x: [token for token in nlp(x)][0].pos_)\n",
    "  artifacts_df['artifact_length'] = artifacts_df['artifact'].apply(len)\n",
    "#   artifacts_df = artifacts_df[artifacts_df['pos'] == 'ADV']\n",
    "  artifacts_df.sort_values(by='count', inplace=True, ascending=False)\n",
    "  artifacts_df.reset_index(drop=True, inplace=True)\n",
    "  pickle.dump(artifacts_df, open(f'{data_dir_main}/adv_artifacts.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50478781",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T18:08:21.004635Z",
     "start_time": "2022-07-28T18:08:20.630383Z"
    }
   },
   "outputs": [],
   "source": [
    "artifacts_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7df57d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T20:06:44.655415Z",
     "start_time": "2022-07-21T20:06:44.623367Z"
    }
   },
   "outputs": [],
   "source": [
    "minimum,maximum = min(artifacts_df['count']), max(artifacts_df['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed96c3ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T20:06:44.695910Z",
     "start_time": "2022-07-21T20:06:44.656709Z"
    }
   },
   "outputs": [],
   "source": [
    "artifacts_df[(artifacts_df['count'] == minimum) & (artifacts_df['artifact_length'] == 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d928db4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T20:06:44.728109Z",
     "start_time": "2022-07-21T20:06:44.697179Z"
    }
   },
   "outputs": [],
   "source": [
    "artifacts_df[(artifacts_df['count'] == maximum) & (artifacts_df['artifact_length'] == 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238b6cf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T20:10:06.010937Z",
     "start_time": "2022-07-21T20:10:05.653188Z"
    }
   },
   "outputs": [],
   "source": [
    "print(artifacts_df[(artifacts_df['count'] == minimum) & (artifacts_df['artifact_length'] == 4)]['artifact'].sample().values[0])\n",
    "print(artifacts_df[(artifacts_df['count'] == maximum) & (artifacts_df['artifact_length'] == 4)]['artifact'].sample().values[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
